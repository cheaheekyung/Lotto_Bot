from django.conf import settings
import os
from glob import glob
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough



OPENAI_API_KEY = settings.OPENAI_API_KEY


def load_documents():
    """
    Description: ../data í´ë”ì—ìˆëŠ” ëª¨ë“  pdfë¬¸ì„œë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜

    - PyPDFLoaderë¥¼ ì‚¬ìš©í•´ ë¡œë“œ
    - ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥í• ë•Œ RecursiveCharacterTextSplitter ë¥¼ ì‚¬ìš©í•˜ë‹ˆ load_and_split ë§ê³  ê·¸ëƒ¥ load ë§Œ ì‚¬ìš©
    """
    file_paths = glob(os.path.join('data/', '*.pdf'))
    print(file_paths)
    documents = []
    
    for file_path in file_paths:
        loader = PyPDFLoader(file_path)
        documents += loader.load()
    return documents


def create_vectorstore():
    """
    Description: ë¬¸ì„œë¥¼ ì„ë² ë”©í•œ í›„ ë²¡í„°DBì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    
    - Chroma DBë¥¼ ì‚¬ìš©
    - ê¸°ì¡´ ë²¡í„°ì €ì¥ì†Œê°€ ìˆë‹¤ë©´ ë¶ˆëŸ¬ì˜¤ê³  ì—†ë‹¤ë©´ ìƒˆë¡œìƒì„±
    """
    persist_directory = os.path.join(settings.BASE_DIR, "vector_store")
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small",)
    
    if os.path.exists(persist_directory):
        print("âœ… Using existing vector store.")
        return Chroma(persist_directory=persist_directory, embedding_function=embeddings)
    else:
        documents = load_documents()
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", " "]
        )
        split_docs = text_splitter.split_documents(documents)
        
        vector_store = Chroma.from_documents(
            documents=split_docs,
            embedding=embeddings,
            persist_directory=persist_directory,
        )
        print("ğŸ”„ Creating new vector store...")
        return vector_store

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

def create_response(message):
    """
    Description: messageë¥¼ ë°›ìœ¼ë©´ ragì ìš©í›„ ë²¡í„°DBì—ì„œ ìœ ì‚¬ë„ë†’ì€ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ì•„ ë‹µë³€í•´ì£¼ëŠ” í•¨ìˆ˜
    
    - LCEL ì ìš©
    - í”„ë¡¬í”„íŒ… ë°©ë²• ê³ ì³ë‚˜ê°€ì•¼í•¨
    """
    
    vector_store = create_vectorstore()
    retriever = vector_store.as_retriever(search_kwargs={"k": 10})
    
    llm = ChatOpenAI(model="gpt-4o-mini",temperature=0)
    prompt = ChatPromptTemplate.from_template("""
        ë¡œë˜ ê´€ë ¨ ì§ˆë¬¸ì´ ë“¤ì–´ì˜¤ë©´ ì•„ë˜ contextsë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µë³€í•´.
        ì¶”ì²œì„ í•´ë‹¬ë¼ê³ í•˜ë©´ ì„¤ëª…ê³¼í•¨ê»˜ ë‹¹ì²¨í™•ë¥ ì´ ë†’ì€ ë²ˆí˜¸ë¥¼ ë‹µë³€í•´.
        ë‹µë³€ì„ í• ë•Œì—ëŠ” ì´ìœ ì™€ í•¨ê»˜ ì„¤ëª…í•´ì¤˜ì•¼í•´.
        ì´ìœ ëŠ” ê°„ê²°í•˜ê²Œ ì„¤ëª…í•´ì¤˜.
        
        contexts: {context}
        ì§ˆë¬¸: {question}
        ë‹µë³€: 
    """)
    
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    response = rag_chain.invoke(message)
    return response

